{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 2.9. Model Selection and the Bias-Variance Tradeoff\n",
    "\n",
    "### Review\n",
    "\n",
    "Models discussed so far have a *smoothing* or *complexity* parameter that has to be determined.\n",
    "* The multiplier of the penalty term:  \n",
    "The parameter $\\lambda$ indexes models ranging from a straight line fit to the interpolating model.\n",
    "* The number of basis functions:  \n",
    "A local degree-$m$ polynomial model ranges between a degree-$m$ global polynomial when the window size is infinitely large, to an interpolating fit when the window size shrinks to zero.\n",
    "* The width of the kernel\n",
    "\n",
    "This means that we cannot use residual sum-of-squares on the training data to determine these parameters as well, since we would always pick those that gave interpolating fits and hence zero residuals. Such a model is unlikely to predict future data well at all.\n",
    "\n",
    "### The bias-variance tradeoff for the KNN\n",
    "\n",
    "Suppose a model $Y=f(X)+\\epsilon$, with $\\text{E}(\\epsilon)=0$ and $\\text{Var}(\\epsilon)=\\sigma^2$, and the KNN estimator $\\hat{f}_k$. And assuming for simplicity that $X$ is nonrandom, the expected prediction error at $x_0$, a.k.a. *test* or *generalization* error, can be decomposed:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{EPE}_k(x_0) &= \\text{E}\\left[(Y-\\hat{f}_k(x_0))^2|X=x_0\\right] \\\\\n",
    "&= \\text{E}\\left[(Y -f(x_0) + f(x_0) -\\hat{f}_k(x_0))^2|X=x_0\\right] \\\\\n",
    "&= \\text{E}(\\epsilon^2) + 2\\text{E}\\left[\\epsilon(f(x_0) -\\hat{f}_k(x_0))|X=x_0\\right] + \\text{E}\\left[\\left(f(x_0)-\\hat{f}_k(x_0)\\right)^2|X=x_0\\right] \\\\\n",
    "&= \\sigma^2 + 0+ \\left[\\text{Bias}^2(\\hat{f}_k(x_0))+\\text{Var}_\\mathcal{T}(\\hat{f}_k(x_0))\\right] \\\\\n",
    "&= \\sigma^2 + \\left(f(x_0) - \\frac{1}{k}\\sum_{l=1}^k f(x_{(l)})\\right)^2 + \\frac{\\sigma^2}{k}\n",
    "\\end{align}\n",
    "\n",
    "* The first term $\\sigma^2$ is the *irreducible* error -- the variance of the new test target -- and is beyond our control, even if we know the true $f(x_0)$.\n",
    "* The second and third terms are from the bias-variance decomposition and are under our control.\n",
    "* The bias will most likely increase with $k$, if the true function is reasonably smooth.\n",
    "* The variance term decreases as the inverse of $k$.\n",
    "\n",
    "So as $k$ varies, there is a *bias-variance tradeoff*.\n",
    "\n",
    "More generally, as the *model complexity* of our procedure is increased, the variance tends to increase and the bias thends to decrease. For KNN, the model complexity is controlled by $k$.\n",
    "\n",
    "Please check FIGURE 2.11 in the textbook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
