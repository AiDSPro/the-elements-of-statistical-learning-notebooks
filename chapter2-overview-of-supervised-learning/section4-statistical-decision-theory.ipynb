{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 2.4. Statistical Decision Theory\n",
    "\n",
    "> The best prediction of $Y$ at any point $X=x$ is the conditional mean, when best is measured by average squared error.\n",
    "\n",
    "Let $X\\in\\mathbb{R}^p$ denote a real valued random input vector, and $Y\\in\\mathbb{R}$ a real valued random output variable, with joint distribution $\\text{Pr}(X,Y)$. We seek a function $f(X)$ for predicting $Y$  given values of the input $X$. This Theory requires a *loss function* $L(Y, f(X))$ for penalizing errors in prediction, and by for the most common and convenient is *squared error loss*:\n",
    "\n",
    "\\begin{equation}\n",
    "L(Y, f(X)) = (Y-f(X))^2.\n",
    "\\end{equation}\n",
    "\n",
    "This leads us to a criterion for choosing $f$ the expected (squared) prediction error:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{EPE}(f) &= \\text{E}(Y-f(X))^2\\\\\n",
    "&= \\int\\left(y-f(x)\\right)^2\\text{Pr}(dx,dy),\n",
    "\\end{align}\n",
    "\n",
    "By conditioning on $X$, we can write EPE as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{EPE}(f) = \\text{E}_X\\text{E}_{Y|X}\\left(\\left[Y-f(X)\\right]^2|X\\right)\n",
    "\\end{equation}\n",
    "\n",
    "and we see that it suffices to minimize EPE pointwise:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\arg\\min_c\\text{E}_{Y|X}\\left(\\left[Y-c\\right]^2|X=x\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The solution is the conditional expectation a.k.a. the *regression* function:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\text{E}\\left(Y|X=x\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions first\n",
    "\n",
    "Both KNN and least squares will end up approximating conditional expectations by averages. But they differ dramatically in terms of model assumptions:\n",
    "* Least squares assumes $f(x)$ is well approximated by a globally linear function.\n",
    "* $k$-nearest neighbors assumes $f(x)$ is well approximated by a locally constant function.\n",
    "Although the latter seems more palatable, we will see below that we may pay a price for this flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The light and shadows of KNN\n",
    "\n",
    "The KNN attempts to directly implement this recipe using the training data:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x) = \\text{Ave}\\left(y_i|x_i\\in N_k(x)\\right),\n",
    "\\end{equation}\n",
    "\n",
    "where \"Ave\" denotes average. Two approximations are happening here:\n",
    "* Expectation is approximated by averaging over sample data;\n",
    "* Conditioning at a point is relaxed to conditioning on some region \"close\" to the target point.\n",
    "\n",
    "Note that under mild regularity conditions on the joint probability distribution $\\text{Pr}(X,Y)$, one can show that\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x)\\rightarrow\\text{E}(Y|X=x)\\text{ as }N,k\\rightarrow\\infty\\text{ s.t. }k/N\\rightarrow0\n",
    "\\end{equation}\n",
    "\n",
    "In light of this, why look further, since it seems we have a universal approximator?\n",
    "* We often do not have very large samples. Linear models can usually get a more stable estimate than $k$-nearest neighbors, provided the structured model is appropriate.\n",
    "* Curse of dimensionality: As the dimension $p$ gets large, so does the metric size of the $k$-nearest neighborhood. The convergence above still holds, but the *rate* of convergence decreases as the dimension increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based approach for linear regression\n",
    "\n",
    "Model-based approach specifies a model for the regression function. Assume that the regression function $f(x)$ is approximately linear in its arguments:\n",
    "\\begin{equation}\n",
    "f(x)\\approx x^T\\beta\n",
    "\\end{equation}\n",
    "\n",
    "Plugging this linear model for $f(x)$ into EPE and differentiating, we can solve for $\\beta$ theoretically:\n",
    "\\begin{equation}\n",
    "\\beta = \\left[\\text{E}\\left(XX^T\\right)\\right]^{-1}\\text{E}(XY)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes classifier: for a categorical output with 0-1 loss function\n",
    "\n",
    "For a catagorical variable $G$ with $\\mathcal{G}$, a set of possible classes, we need a different loss function for penalizing prediction errors.\n",
    "\n",
    "Our loss function can be represented by $K\\times K$ matrix $\\mathbf{L}$, where $K=\\text{card}(\\mathcal{G})$. $\\mathbf{L}$ will be zero on the diagonal and nonnegative elsewhere, representing the price paid for misclassifying $\\mathcal{G}_k$ as $\\mathcal{G}_l$.\n",
    "\n",
    "Most often we use the *zero-one* loss function, where all misclassifications are charged a single unit, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "L(k,l) = \\begin{cases}\n",
    "0\\text{ if }k = l,\\\\\n",
    "1\\text{ otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The expected prediction error is\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{EPE} = \\text{E}\\left[L(G, \\hat{G}(X)\\right],\n",
    "\\end{equation}\n",
    "\n",
    "where the expectation is taken w.r.t. the joint distribution $\\text{Pr}(G, X)$. Again we condition, and can write EPE as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{EPE} = \\text{E}_X\\sum^K_{k=1}L\\left(\\mathcal{G}_k, \\hat{G}(X)\\right)\\text{Pr}\\left(\\mathcal{G}_k|X\\right)\n",
    "\\end{equation}\n",
    "\n",
    "and again it suffices to minimize EPE pointwise:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{G}(x) = \\arg\\min_{g\\in\\mathcal{G}}\\sum^K_{k=1} L\\left(\\mathcal{G}_k,g\\right)\\text{Pr}\\left(\\mathcal{G}_k|X=x\\right)\n",
    "\\end{equation}\n",
    "\n",
    "With the 0-1 loss function this simplifies to\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{G}(x) &= \\arg\\min_{g\\in\\mathcal{G}} \\left[1 - \\text{Pr}(g|X=x)\\right]\\\\\n",
    "&= \\mathcal{G}_k \\text{ if Pr}\\left(\\mathcal{G}_k|X=x\\right) = \\max_{g\\in\\mathcal{G}}\\text{Pr}(g|X=x)\n",
    "\\end{align}\n",
    "\n",
    "This reasonable solution is known as the *Bayes classifier*, and says that we classify to the most probable class, using the conditional (discrete) distribution $\\text{Pr}(G|X)$. The error rate of the Bayes classifier is called the *Bayes rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN and Bayes classifier\n",
    "\n",
    "Again we see that the $k$-nearest neighbor classifier directly approximates this solution -- a majority vote in a nearest neighborhood amounts to exactly this, except that\n",
    "* conditional probability at a point is relaxed to conditional probability within a neighborhood of a point,\n",
    "* and probabilities are estimated by training-sample proportions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
