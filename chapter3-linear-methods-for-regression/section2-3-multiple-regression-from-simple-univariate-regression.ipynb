{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 3.2.3. Multiple Regression from Simple Univariate Regression\n",
    "\n",
    "The linear model with $p \\gt 1$ inputs is called *multiple linear regression model*. The least squares estimates for this model are best understood in terms of the estimates for the *univariate* ($p=1$) linear model, as we indicate in this section.\n",
    "\n",
    "Suppose a univariate model with no intercept,\n",
    "\n",
    "\\begin{equation}\n",
    "Y=X\\beta+\\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "The least squares estimtae and residuals are\n",
    "\n",
    "\\begin{align}\n",
    "\\hat\\beta &= \\frac{\\sum_1^N x_i y_i}{\\sum_1^N x_i^2} = \\frac{\\langle\\mathbf{x},\\mathbf{y}\\rangle}{\\langle\\mathbf{x},\\mathbf{x}\\rangle}, \\\\\n",
    "r_i &= y_i - x_i\\hat\\beta, \\\\\n",
    "\\mathbf{r} &= \\mathbf{y} - \\mathbf{x}\\hat\\beta,\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{y}=(y_1,\\cdots,y_N)^T$, $\\mathbf{x}=(x_1,\\cdots,x_N)^T$ and $\\langle\\cdot,\\cdot\\rangle$ denotes the dot product notation.\n",
    "\n",
    "This simple univariate regression provides the building block for multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks for multiple linear regression\n",
    "\n",
    "Suppose next that the columns of the data matrix $\\mathbf{X} = \\left[\\mathbf{x}_1,\\cdots,\\mathbf{x}_p\\right]$ are orthogonal, i.e., \n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\mathbf{x}_j,\\mathbf{x}_k\\rangle = 0\\text{ for all }j\\neq k.\n",
    "\\end{equation}\n",
    "\n",
    "Then it is easy to check that the multiple least squares estimates $\\hat\\beta_j$ are equal to the univariate estimates $\\frac{\\langle\\mathbf{x}_j,\\mathbf{y}\\rangle}{\\langle\\mathbf{x}_j,\\mathbf{x}_j\\rangle}$. In other words, when the inputs are orthogonal, they have no effect on each other's parameter estimates in the model. In order to carry this idea further, we need to orthogonalize the observational data, in which the orthogonal inputs almost never occur.\n",
    "\n",
    "Suppose next that we have an intercept and a single input $\\mathbf{x}$. Then the least squares coefficient of $\\mathbf{x}$ has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta_1 = \\frac{\\langle\\mathbf{x}-\\bar{x}\\mathbf{1},\\mathbf{y}\\rangle}{\\langle\\mathbf{x}-\\bar{x}\\mathbf{1},\\mathbf{x}-\\bar{x}\\mathbf{1}\\rangle},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar{x} = \\sum x_i /N$ and $\\mathbf{1} = \\mathbf{x}_0$. And also note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x}\\mathbf{1} = \\frac{\\langle\\mathbf{1},\\mathbf{x}\\rangle}{\\langle\\mathbf{1},\\mathbf{1}\\rangle}\\mathbf{1},\n",
    "\\end{equation}\n",
    "\n",
    "which means the fitted value in the case we regress $\\mathbf{x}$ on $\\mathbf{x}_0=\\mathbf{1}$. Therefore we can view $\\hat\\beta_1$ as the result of two application of the simple regression with the following steps:\n",
    "1. Regress $\\mathbf{x}$ on $\\mathbf{1}$ to produce the residual $\\mathbf{z}=\\mathbf{x}-\\bar{x}\\mathbf{1}$;\n",
    "2. regress $\\mathbf{y}$ on the residual $\\mathbf{z}$ to give the coefficient $\\hat\\beta_1$.\n",
    "\n",
    "> In this procedure, \"regreess $\\mathbf{b}$ on $\\mathbf{a}$\" means a simple univariate regression of $\\mathbf{b}$ on $\\mathbf{a}$ with no intercept, producing coefficient $\\hat\\gamma=\\langle\\mathbf{a},\\mathbf{b}\\rangle/\\langle\\mathbf{a},\\mathbf{a}\\rangle$ and residual vector $\\mathbf{b}-\\hat\\gamma\\mathbf{a}$. We say that $\\mathbf{b}$ is adjusted for $\\mathbf{a}$, or is \"orthogonalized\" w.r.t. $\\mathbf{a}$.\n",
    "\n",
    "In other words,\n",
    "1. orthogonalize $\\mathbf{x}$ w.r.t. $\\mathbf{x}_0=\\mathbf{1}$;\n",
    "2. just a simple univariate regression, using the orthogonal predictors $\\mathbf{1}$ and $\\mathbf{z}$.\n",
    "\n",
    "The orthogonalization does not change the subspace spanned by $\\mathbf{x}_0$ and $\\mathbf{x}_1$, it simply produces an orthogonal basis for representing it. See FIGURE 3.4 in the textbook for geometric interpretation.\n",
    "\n",
    "This recipe gerenalizes to the case of $p$ inputs.\n",
    "\n",
    "#### Algorithm 3.1. Regression by successive orthogonalization\n",
    "1. Initialize $\\mathbf{z}_0=\\mathbf{x}_0=\\mathbf{1}$.\n",
    "2. For $j = 1, 2, \\cdots, p$,  \n",
    "  regress $\\mathbf{x}_j$ on $\\mathbf{z}_0,\\mathbf{z}_1,\\cdots,\\mathbf{z}_{j-1}$ to produce\n",
    "  * coefficients $\\hat\\gamma_{lj} = \\langle\\mathbf{z}_l,\\mathbf{x}_j\\rangle/\\langle\\mathbf{z}_l,\\mathbf{z}_l\\rangle$, for $l=0,\\cdots, j-1$ and\n",
    "  * residual vector $\\mathbf{z}_j=\\mathbf{x}_j - \\sum_{k=0}^{j-1}\\hat\\gamma_{kj}\\mathbf{z}_k$.\n",
    "3. Regress $\\mathbf{y}$ on the residual $\\mathbf{z}_p$ to give the estimate $\\hat\\beta_p$,\n",
    "\\begin{equation}\n",
    "\\hat\\beta_p = \\frac{\\langle\\mathbf{z}_p,\\mathbf{y}\\rangle}{\\langle\\mathbf{z}_p,\\mathbf{z}_p\\rangle}.\n",
    "\\end{equation}\n",
    "\n",
    "In step 2, $\\mathbf{x}_j = \\mathbf{z}_j + \\sum\\hat\\gamma_{kj}\\mathbf{z}_k$, which is a linear combination of the $\\mathbf{z}_k$ for $k\\le j$. Since the $\\mathbf{z}_j$ are all orthogonal, they form a basis for the $\\text{col}(\\mathbf{X})$, and hence the least squares projection onto this subspace is $\\hat{\\mathbf{y}}$. Since $\\mathbf{z}_p$ alone involves $\\mathbf{x}_p$ (with coefficient 1), we see that the coefficient $\\hat\\beta_p$ is indeed the multiple regression coefficient of $\\mathbf{y} on \\mathbf{x}_p$. This key result exposes the effect of correlated inputs in mutiple regression.\n",
    "\n",
    "> The multiple regression coefficient $\\hat\\beta_j$ represents the additional contribution of $\\mathbf{x}_j$ on $\\mathbf{y}$, after $\\mathbf{x}_j$ has been adjusted for $\\mathbf{x}_0,\\mathbf{x}_1,\\cdots,\\mathbf{x}_{j-1},\\mathbf{x}_{j+1},\\cdots,\\mathbf{x}_p$.\n",
    "\n",
    "We obtain an alternative formula for the variance estimates\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(\\hat\\beta_p) = \\text{Var}\\left(\\frac{\\langle\\mathbf{z}_p,\\mathbf{y}\\rangle}{\\langle\\mathbf{z}_p,\\mathbf{z}_p\\rangle}\\right) = \\frac{\\sigma^2}{\\|\\mathbf{z}_p\\|^2},\n",
    "\\end{equation}\n",
    "\n",
    "implying that the precision with which we can estimate with $\\hat\\beta_p$ depends on the length of the residual vector $\\mathbf{z}_p$; this represents how much of $\\mathbf{x}_p$ is unexplained by the other $\\mathbf{x}_k$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram-Schmidt procedure and QR decomposition\n",
    "\n",
    "Algorithm 3.1 is known as the *Gram-Schmidt* procedure for multiple regression, and is also a useful numerical strategy for computing the estimates. We can represent step 2 of Algorithm 3.1 in matrix form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{Z\\Gamma},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* $\\mathbf{Z}$ has as columns the $\\mathbf{z}_j$ (in order)\n",
    "* $\\mathbf{\\Gamma}$ is the upper triangular matrix with entries $\\hat\\gamma_{kj}$.\n",
    "\n",
    "Introducing the diagonal matrix $\\mathbf{D}$ with $D_{jj}=\\|\\mathbf{z}_j\\|$, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X} &= \\mathbf{ZD}^{-1}\\mathbf{D\\Gamma} \\\\\n",
    "&= \\mathbf{QR},\n",
    "\\end{align}\n",
    "\n",
    "the so-called *QR decomposition* of $\\mathbf{X}$. Here\n",
    "* $\\mathbf{Q}$ is an $N\\times(p+1)$ orthogonal matrix s.t. $\\mathbf{Q}^T\\mathbf{Q}=\\mathbf{I}$,\n",
    "* $\\mathbf{R}$ is a $(p+1)\\times(p+1)$ upper triangular matrix.\n",
    "\n",
    "The QR decomposition represents a convenient orthogonal basis for the $\\text{col}(\\mathbf{X})$. It is easy to see, for example, that the least squares solution is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\hat\\beta &= \\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}, \\\\\n",
    "\\hat{\\mathbf{y}} &= \\mathbf{QQ}^T\\mathbf{y}.\n",
    "\\end{align}\n",
    "\n",
    "Note that the triangular matrix $\\mathbf{R}$ makes it easy to solve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
